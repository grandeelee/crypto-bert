{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from cryptoGPT.model import RNNModel\n",
    "from cryptoGPT.trainer import TrainerConfig\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from war_and_peace_utils import create_logger, CharDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"context_length\", type=int, default=5)\n",
    "parser.add_argument(\"n_layer\", type=int, default=12)\n",
    "parser.add_argument(\"batch_size\", type=int, default=64)\n",
    "parser.add_argument(\"n_expt\", type=int, default=1)\n",
    "arg = parser.parse_args()\n",
    "\n",
    "context_length = arg.context_length\n",
    "num_layer = arg.n_layer\n",
    "batch_size = arg.batch_size\n",
    "n_expt = arg.n_expt\n",
    "\n",
    "if not os.path.exists(\"war_and_peace_expt_lstm\"):\n",
    "    os.makedirs(\"war_and_peace_expt_lstm\")\n",
    "save_path = \"war_and_peace_expt_lstm/l{}_c{}\".format(num_layer, context_length)\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "log_path = os.path.join(save_path, \"log\")\n",
    "\n",
    "logger = create_logger(log_path=log_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 3227580 characters\n",
      "data has 3227580 characters\n"
     ]
    }
   ],
   "source": [
    "with open('war_and_peace.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "word2idx = {j: i for i, j in enumerate(chars)}\n",
    "idx2word = {i: j for i, j in enumerate(chars)}\n",
    "pkey = np.random.permutation(vocab_size)\n",
    "ipkey = np.argsort(pkey)\n",
    "indexed_text = [word2idx[i] for i in text]\n",
    "permuted_text = [pkey[i] for i in indexed_text]\n",
    "\n",
    "train_dataset = CharDataset(indexed_text, context_length)\n",
    "test_dataset = CharDataset(permuted_text, context_length)\n",
    "\n",
    "# init a new model\n",
    "model = RNNModel('LSTM', vocab_size, 64, 64, num_layer)\n",
    "params = [p for pn, p in model.named_parameters() if p.requires_grad]\n",
    "save_original_path = os.path.join(save_path, \"original.pth\")\n",
    "tconf = TrainerConfig(max_epochs=2, batch_size=batch_size, learning_rate=6e-4,\n",
    "                      ckpt_path=save_original_path,\n",
    "                      lr_decay=True, warmup_tokens=512 * 20,\n",
    "                      final_tokens=2 * len(train_dataset) * context_length,\n",
    "                      num_workers=4)\n",
    "optimizer = torch.optim.Adam(params, lr=tconf.learning_rate, betas=tconf.betas)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    model = torch.nn.DataParallel(model).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "# Turn on training mode which enables dropout.\n",
    "\n",
    "def train(model, optimizer):\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    loader = DataLoader(train_dataset, shuffle=True, pin_memory=True,\n",
    "                                batch_size=batch_size,\n",
    "                                num_workers=4)\n",
    "    pbar = tqdm(enumerate(loader), total=len(loader))\n",
    "\n",
    "    tokens = 0\n",
    "    for epoch in range(tconf.max_epochs):\n",
    "        for it, (x, y) in pbar:\n",
    "            # place data on the correct device\n",
    "            x = x.to(device).permute(1, 0)\n",
    "            y = y.to(device).permute(1, 0).reshape(-1)\n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            model.zero_grad()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output, hidden = model(x, hidden)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            tokens += (y >= 0).sum()  # number of tokens processed this step (i.e. label is not -100)\n",
    "            if tokens < tconf.warmup_tokens:\n",
    "                # linear warmup\n",
    "                lr_mult = float(tokens) / float(max(1, tconf.warmup_tokens))\n",
    "            else:\n",
    "                # cosine learning rate decay\n",
    "                progress = float(tokens - tconf.warmup_tokens) / float(\n",
    "                    max(1, tconf.final_tokens - tconf.warmup_tokens))\n",
    "                lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "            lr = tconf.learning_rate * lr_mult\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            pbar.set_description(f\"epoch {epoch + 1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
    "\n",
    "train(model, optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "char_emb = model.encoder.weight.data\n",
    "emb = char_emb.to('cpu').numpy()\n",
    "emb /= np.linalg.norm(emb, axis=1, keepdims=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avg_result = []\n",
    "for i in range(n_expt):\n",
    "    # reinit the embedding of the model\n",
    "    model.encoder.weight.data.uniform_(-0.1, 0.1)\n",
    "    # re-initialize a trainer instance and kick off training\n",
    "    save_new_path = os.path.join(save_path, \"new_{}.pth\".format(i))\n",
    "    tconf = TrainerConfig(max_epochs=2, batch_size=batch_size, learning_rate=6e-4,\n",
    "                          ckpt_path=save_new_path,\n",
    "                          lr_decay=True, warmup_tokens=512 * 20,\n",
    "                          final_tokens=2 * len(test_dataset) * context_length,\n",
    "                          num_workers=4)\n",
    "    for pn, p in model.named_parameters():\n",
    "        if \"encoder\" not in pn:\n",
    "            p.requires_grad = False\n",
    "    params = [p for pn, p in model.named_parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(params, lr=tconf.learning_rate, betas=tconf.betas)\n",
    "    train(model, optimizer)\n",
    "    #  store the embedding\n",
    "    char_emb_p = model.encoder.weight.data\n",
    "    emb_p = char_emb_p.to('cpu').numpy()\n",
    "    # normalize embedding for self similarity\n",
    "    emb_p /= np.linalg.norm(emb_p, axis=1, keepdims=True)\n",
    "    # get the restored key\n",
    "    ssm = emb_p @ emb.T\n",
    "    restored_key = np.argmax(ssm, axis=1)\n",
    "    acc = sum(ipkey == restored_key) / len(ipkey) * 100\n",
    "\n",
    "    avg_result.append(acc)\n",
    "    logging.info(\"for layer: {}, context length: {}, run: {}, acc :{}\".format(num_layer, context_length, i, acc))\n",
    "\n",
    "logging.info(\"for layer: {}, context length: {}, ave acc :{}\".format(num_layer, context_length,\n",
    "                                                                     sum(avg_result) / len(avg_result)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}